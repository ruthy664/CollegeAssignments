{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03062dd",
   "metadata": {},
   "source": [
    "## Homework 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115fdff7-3206-42af-a8d0-6082a4062c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc577f",
   "metadata": {},
   "source": [
    "For some dimension $d$, suppose that we have data $\\mathcal S = \\{(\\textbf{x}_i, y_i)\\}_{i=1}^n$, with each $\\textbf{x}_i\\in\\mathbb R^d$ and $y_i\\in\\mathbb R$. For each $i$, write the vector $\\textbf{x}_i$ as \n",
    "$$\\textbf{x}_i = (x_{i,1},x_{i,2},\\ldots,x_{i,d}).$$\n",
    "To do ridge regression (linear regression with $L_2$ regularization), we would minimize the loss function \n",
    "$$\\mathcal L_{\\mathcal S}(\\textbf{w}, b) = \\lambda|\\textbf{w}|^2 + \\frac{1}{n}\\sum_{i=1}^n(\\textbf{w}\\cdot\\textbf{x}_i + b - y_i)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d96af5",
   "metadata": {},
   "source": [
    "1. (a) Having the vector of coefficients $\\textbf{w} = (w_1,w_2,\\ldots,w_d)$, use the notation here to write the partial derivatives $\\frac{\\partial}{\\partial w_j}\\mathcal L_{\\mathcal S}$, for $1\\le j\\le d$, as well as the partial derivative $\\frac{\\partial}{\\partial b}\\mathcal L_{\\mathcal S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b443745",
   "metadata": {},
   "source": [
    "1. (b) Let $I$ be the $d\\times d$ identity matrix. Given $j$, with $1\\le j\\le d$, observe that $2\\lambda w_j$ is equal to the $j^{th}$ entry in the vector $(2\\lambda I)\\textbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11832f2",
   "metadata": {},
   "source": [
    "1. (c) Let $A$ denote the $n\\times(d+1)$ \"feature\" matrix which has entries $(x_{i,1}, \\ldots, x_{i,d}, 1)$ in row $i$. Thinking about entries in $A^TA$ and $A^T\\textbf{y}$, write a matrix equation that represents the system of equations \n",
    "$$\\frac{\\partial}{\\partial w_1}\\mathcal L_{\\mathcal S} = 0,\\quad \n",
    "\\frac{\\partial}{\\partial w_2}\\mathcal L_{\\mathcal S} = 0, \\  \\ldots,\\  \n",
    "\\frac{\\partial}{\\partial w_d}\\mathcal L_{\\mathcal S}=0,\\quad \n",
    "\\textrm{and}\\quad \\frac{\\partial}{\\partial b}\\mathcal L_{\\mathcal S}=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ab2d3",
   "metadata": {},
   "source": [
    "Included with the homework notebook are two CSV files: `'train_HW5data.csv'` and `'test_HW5data.csv'`. These CSV files have a column 'x' and a column 'y'. Read each of the data sets into Python.\n",
    "\n",
    "2. (a) If `x_train` is an array containing data in column 'x' from the train data, make an array that has 12 columns, so that the columns are `x_train**12`, `x_train**11`, ... etc. The shape of the resulting array should be `(40, 12)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d573bb1d-7945-4a7c-9f77-ea59d8838fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_HW5data.csv')\n",
    "test = pd.read_csv('test_HW5data.csv')\n",
    "\n",
    "x_train = np.array(train['x'])\n",
    "y_train = np.array(train['y'])\n",
    "x_test = np.array(test['x'])\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "x_train12 = np.array([x_train**i for i in range(12,0,-1)]).T\n",
    "x_test12 = np.array([x_test**i for i in range(12,0,-1)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039f1ec",
   "metadata": {},
   "source": [
    "\n",
    "2. (b) Import the Class `Ridge` from the scikit-learn submodule `sklearn.linear_model`. When initializing the class, set the `alpha` (hyper)parameter equal to 0.01 (this is what we called $\\lambda$ when describing regularization in class). Read about the `solver` methods for this class on [the documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge). Set your class instance to use a solver that performs a type of gradient descent. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Next, using `y_train` for the array with data from column 'y', train your instance of `Ridge` (use the method `.fit()`) on the array you made in (a), with `y_train` as the labels. Print out the coefficients of the resulting model and compute the Mean Squared Error on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a92200f-f329-45f4-bce9-47bedb933b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: [ 0.11675336  0.04255559 -0.02180575 -0.06889048 -0.0900023  -0.07683982\n",
      " -0.02546185  0.05638509  0.14086825  0.18839607  0.2252151   0.36029699]\n",
      "Mean Squared Error: 0.016457457155559636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "model = Ridge(alpha = 0.01, solver = 'sag', max_iter=10000) \n",
    "model.fit(x_train12, y_train)\n",
    "print('coefficients:', model.coef_)\n",
    "\n",
    "y_pred = model.predict(x_test12)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7611c",
   "metadata": {},
   "source": [
    "3. Import the Class `Lasso` from the scikit-learn submodule `sklearn.linear_model` (here is the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)) and create an instance of the class with `alpha = 0.005`.  As in number 2, use the train and test data sets to fit the Lasso model to find a degree 12 polynomial on 'x' with the 'y' column as the response. For this model, which powers of 'x' have non-zero coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69020bae-9307-4526-b03d-1032e2b3c947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.58805528 0.17271755]\n",
      "Non-zero coefficient powers of x: [2, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "model2 = Lasso(alpha = 0.005)\n",
    "model2.fit(x_train12, y_train)\n",
    "print(\"Coefficients:\", model2.coef_)\n",
    "\n",
    "nonzero_powers = [12 - i for i, coef in enumerate(model2.coef_) if coef != 0]\n",
    "print(\"Non-zero coefficient powers of x:\", nonzero_powers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
